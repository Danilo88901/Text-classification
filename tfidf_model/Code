# Upload and extract the dataset
from google.colab import files
uploaded = files.upload()

import zipfile
import os

# Unzip the main archive
with zipfile.ZipFile("jigsaw-toxic-comment-classification-challenge.zip", 'r') as zip_ref:
    zip_ref.extractall("data")

print("Contents after extracting main archive:", os.listdir("data"))

# Unzip train.csv.zip
train_zip_path = "data/train.csv.zip"
with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:
    zip_ref.extractall("data")

print("Contents after extracting train.csv.zip:", os.listdir("data"))

# Load training data
import pandas as pd
train_df = pd.read_csv("data/train.csv")
train_df.head()



# Split data into training and validation sets
  
from sklearn.model_selection import train_test_split

texts = train_df['comment_text'].values
labels = train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values

X_train0, X_val0, y_train_np, y_val_np = train_test_split(
    texts, labels, test_size=0.065, random_state=42
)

print(f"Train size: {len(X_train0)}, Validation size: {len(X_val0)}")


 # Text preprocessing and vectorization

  
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer

import torch
from torch import nn
from sklearn.metrics import roc_auc_score

import matplotlib.pyplot as plt

# Download NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

# Setup preprocessing tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
tokenizer = RegexpTokenizer(r'\w+')

# Use GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Device:", device)

# Preprocessing function
def preprocess(text):
    tokens = tokenizer.tokenize(text.lower())
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]
    return " ".join(tokens)

# Clean text data
X_train_cleaned = [preprocess(t) for t in X_train0]
X_val_cleaned = [preprocess(t) for t in X_val0]
# TF-IDF vectorization
vectorizer = TfidfVectorizer(max_features=20000)
X_train_arr = vectorizer.fit_transform(X_train_cleaned).toarray()
X_val_arr = vectorizer.transform(X_val_cleaned).toarray()

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train_arr, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val_arr, dtype=torch.float32)
y_train = torch.tensor(y_train_np, dtype=torch.float32)
y_val = torch.tensor(y_val_np, dtype=torch.float32)
  # Neural network model
class Model(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size * 2),
            nn.BatchNorm1d(hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size * 2, num_classes)
        )

    def forward(self, x):
        return self.fc(x)

# Initialize model and training setup
model = Model(20000, 512, 6).to(device)
loss_fn = nn.BCEWithLogitsLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)
  # Training loop
epochs = 150
early_stop_patience = 7
patience_counter = 0
best_roc_auc = 0

# Move tensors to device
X_train_tensor = X_train_tensor.to(device)
y_train = y_train.to(device)
X_val_tensor = X_val_tensor.to(device)
y_val = y_val.to(device)

roc_aucs_train = []
roc_aucs_val = []
losses_train = []
losses_val = []

for epoch in range(epochs):
    # Training phase
    model.train()
    logits_train = model(X_train_tensor)
    loss_train = loss_fn(logits_train, y_train)
    probs_train = torch.sigmoid(logits_train).detach().cpu().numpy()
    y_train_np = y_train.cpu().numpy()
    train_roc_auc = roc_auc_score(y_train_np, probs_train, average='macro')

    optimizer.zero_grad()
    loss_train.backward()
    optimizer.step()

    # Validation phase
    model.eval()
    with torch.inference_mode():
        logits_val = model(X_val_tensor)
        loss_val = loss_fn(logits_val, y_val)
        probs_val = torch.sigmoid(logits_val).cpu().numpy()
        y_val_np = y_val.cpu().numpy()
        val_roc_auc = roc_auc_score(y_val_np, probs_val, average='macro')

        scheduler.step(val_roc_auc)

        # Save best model
        if val_roc_auc > best_roc_auc:
            best_roc_auc = val_roc_auc
            torch.save(model.state_dict(), "best_model.pt")
            print(f"Model saved at epoch {epoch+1} with Val ROC AUC: {val_roc_auc:.4f}")
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= early_stop_patience:
            print(f"Early stopping triggered at epoch {epoch+1}")
            break

    # Store metrics
    roc_aucs_train.append(train_roc_auc)
    roc_aucs_val.append(val_roc_auc)
    losses_train.append(loss_train.item())
    losses_val.append(loss_val.item())

    print(
        f"Epoch {epoch+1:02d} | "
        f"Train Loss: {loss_train:.4f}, Train ROC AUC: {train_roc_auc:.4f} | "
        f"Val Loss: {loss_val:.4f}, Val ROC AUC: {val_roc_auc:.4f}"
    )
  # Plot training and validation metrics
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(roc_aucs_train, label='Train ROC AUC')
plt.plot(roc_aucs_val, label='Val ROC AUC')
plt.legend()
plt.title('ROC AUC over Epochs')

plt.subplot(1, 2, 2)
plt.plot(losses_train, label='Train Loss')
plt.plot(losses_val, label='Val Loss')
plt.legend()
plt.title('Loss over Epochs')

plt.show()

  # Load test dataset
test_df = pd.read_csv("data/test.csv.zip")

# Preprocess test comments
X_test_cleaned = [preprocess(t) for t in test_df['comment_text']]

# Transform text to TF-IDF features
X_test_arr = vectorizer.transform(X_test_cleaned).toarray()

# Convert features to PyTorch tensor and move to device
X_test_tensor = torch.tensor(X_test_arr, dtype=torch.float32).to(device)

# Load the best saved model weights
model.load_state_dict(torch.load("best_model.pt"))
model.eval()

# Make predictions on test set
with torch.inference_mode():
    logits_test = model(X_test_tensor)
    probs_test = torch.sigmoid(logits_test).cpu().numpy()

# Prepare submission DataFrame
submission = pd.DataFrame(
    probs_test,
    columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
)
submission['id'] = test_df['id']

# Reorder columns as required
submission = submission[['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]

# Save submission file
submission.to_csv("submission.csv", index=False)
  
